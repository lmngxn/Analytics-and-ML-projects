{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_ORYgrrdDx"
      },
      "source": [
        "# IT5005 Artificial Intelligence Term Assignment\n",
        "\n",
        "Fill your name and student number below:\n",
        "\n",
        "\n",
        "| Student Number: | Name:                   |\n",
        "|:----------------|:------------------------|\n",
        "| A0002533Y       | Lee Ming Xuan           |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this assignment we hope to achieve the following:\n",
        "\n",
        "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
        "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
        "    \n",
        "As this lab is more challenging than the previous labs, please work in teams of two persons. Please use the respective categories in the LumiNUS Forum under the \"Labs\" Heading to find a partner within your own group.\n",
        "\n",
        "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
        "\n",
        "\n",
        "## 2. Submission Instructions\n",
        "\n",
        "\n",
        "### 2.1 SUBMISSION INSTRUCTIONS\n",
        "\n",
        "Please rename this Jupyter notebook to your student ID (e.g. A1234567Y.ipynb), complete it and submit to Canvas by 12 pm, Sunday 23 April 2023.\n",
        "\n",
        "The folder will close shortly after 12 pm on 23 April, after which you will no longer be able to submit your assignment and you will get 0.\n",
        "\n",
        "\n",
        "## 3. Creating a Dense Network for CIFAR-10\n",
        "\n",
        "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
        "\n",
        "    1. Airplane\n",
        "    2. Automobile\n",
        "    3. Bird\n",
        "    4. Cat\n",
        "    5. Deer\n",
        "    6. Dog\n",
        "    7. Frog\n",
        "    8. Horse\n",
        "    9. Ship\n",
        "    10. Truck\n",
        "    \n",
        "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
        "\n",
        "### 3.1 Loading the Dataset\n",
        "\n",
        "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qwQvgIMrdD0",
        "outputId": "159c3e7d-37b1-43db-a226-8d613beee10c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "def load_cifar10():\n",
        "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
        "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
        "    train_x = train_x.astype('float32')\n",
        "    test_x = test_x.astype('float32')\n",
        "    train_x /= 255.0\n",
        "    test_x /= 255.0\n",
        "    ret_train_y = to_categorical(train_y,10)\n",
        "    ret_test_y = to_categorical(test_y, 10)\n",
        "    \n",
        "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
        "\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = load_cifar10()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKuQrI9rdD2"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 1\n",
        "\n",
        "Explain what the following two  statements do, and where the number \"3072\" came from (2 MARKS):\n",
        "\n",
        "```\n",
        "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
        "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
        "```\n",
        "\n",
        "**ANSWER: It comes from 32 x 32 x 3 which is the datasize of each image. currently it is in a 3D array, in which we are trying to flatten into a to 1D array.**\n",
        "\n",
        "*FOR GRADER: _______ / 2*\n",
        "\n",
        "### 3.2 Building the MLP Classifier\n",
        "\n",
        "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnQbG51xrdD3",
        "outputId": "169fca5d-732b-41bf-dcbc-be372ca38b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 39s 24ms/step - loss: 1.9715 - accuracy: 0.2871 - val_loss: 1.8611 - val_accuracy: 0.3270\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.7825 - accuracy: 0.3602 - val_loss: 1.8683 - val_accuracy: 0.3758\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.7149 - accuracy: 0.3871 - val_loss: 1.7068 - val_accuracy: 0.3955\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.6670 - accuracy: 0.4058 - val_loss: 1.7231 - val_accuracy: 0.3938\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.6387 - accuracy: 0.4192 - val_loss: 1.7336 - val_accuracy: 0.3812\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.6026 - accuracy: 0.4322 - val_loss: 1.7700 - val_accuracy: 0.3844\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 37s 23ms/step - loss: 1.5692 - accuracy: 0.4445 - val_loss: 1.9798 - val_accuracy: 0.3489\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5460 - accuracy: 0.4529 - val_loss: 1.7072 - val_accuracy: 0.4129\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.5254 - accuracy: 0.4603 - val_loss: 1.6484 - val_accuracy: 0.4212\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.5003 - accuracy: 0.4717 - val_loss: 1.6859 - val_accuracy: 0.4155\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.4818 - accuracy: 0.4759 - val_loss: 1.6261 - val_accuracy: 0.4266\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.4562 - accuracy: 0.4851 - val_loss: 1.6884 - val_accuracy: 0.4174\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.4447 - accuracy: 0.4913 - val_loss: 1.6071 - val_accuracy: 0.4444\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.4260 - accuracy: 0.4970 - val_loss: 1.7403 - val_accuracy: 0.4010\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.4055 - accuracy: 0.5025 - val_loss: 1.6632 - val_accuracy: 0.4474\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.3911 - accuracy: 0.5065 - val_loss: 1.7088 - val_accuracy: 0.4294\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.3720 - accuracy: 0.5148 - val_loss: 1.9071 - val_accuracy: 0.4059\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.3534 - accuracy: 0.5188 - val_loss: 1.5762 - val_accuracy: 0.4668\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.3407 - accuracy: 0.5247 - val_loss: 1.5182 - val_accuracy: 0.4820\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.3219 - accuracy: 0.5321 - val_loss: 1.5704 - val_accuracy: 0.4745\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.3068 - accuracy: 0.5400 - val_loss: 1.6680 - val_accuracy: 0.4464\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.2887 - accuracy: 0.5433 - val_loss: 1.8461 - val_accuracy: 0.4268\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.2778 - accuracy: 0.5462 - val_loss: 1.7804 - val_accuracy: 0.4502\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.2609 - accuracy: 0.5555 - val_loss: 1.5583 - val_accuracy: 0.4682\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.2418 - accuracy: 0.5621 - val_loss: 1.6331 - val_accuracy: 0.4609\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.2311 - accuracy: 0.5644 - val_loss: 1.6487 - val_accuracy: 0.4702\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.2156 - accuracy: 0.5690 - val_loss: 1.6964 - val_accuracy: 0.4616\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.1978 - accuracy: 0.5782 - val_loss: 1.6375 - val_accuracy: 0.4592\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.1820 - accuracy: 0.5811 - val_loss: 1.9288 - val_accuracy: 0.4147\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.1707 - accuracy: 0.5869 - val_loss: 1.6446 - val_accuracy: 0.4863\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.1564 - accuracy: 0.5919 - val_loss: 1.6274 - val_accuracy: 0.4848\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.1336 - accuracy: 0.6016 - val_loss: 1.7221 - val_accuracy: 0.4530\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.1251 - accuracy: 0.6023 - val_loss: 1.8529 - val_accuracy: 0.4510\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.1134 - accuracy: 0.6051 - val_loss: 1.6902 - val_accuracy: 0.4927\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.0960 - accuracy: 0.6102 - val_loss: 1.7396 - val_accuracy: 0.4760\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.0799 - accuracy: 0.6203 - val_loss: 1.7290 - val_accuracy: 0.4864\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.0719 - accuracy: 0.6213 - val_loss: 1.6990 - val_accuracy: 0.4860\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.0541 - accuracy: 0.6260 - val_loss: 1.7356 - val_accuracy: 0.4795\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.0426 - accuracy: 0.6317 - val_loss: 1.7380 - val_accuracy: 0.4787\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.0346 - accuracy: 0.6314 - val_loss: 1.8933 - val_accuracy: 0.4693\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 1.0185 - accuracy: 0.6397 - val_loss: 1.9571 - val_accuracy: 0.4621\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.0063 - accuracy: 0.6443 - val_loss: 1.8562 - val_accuracy: 0.4710\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 0.9927 - accuracy: 0.6474 - val_loss: 1.8419 - val_accuracy: 0.4897\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9766 - accuracy: 0.6528 - val_loss: 1.7927 - val_accuracy: 0.4927\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.9677 - accuracy: 0.6603 - val_loss: 1.8557 - val_accuracy: 0.4752\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.9571 - accuracy: 0.6604 - val_loss: 1.9597 - val_accuracy: 0.4762\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9393 - accuracy: 0.6666 - val_loss: 1.8622 - val_accuracy: 0.4877\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 37s 23ms/step - loss: 0.9351 - accuracy: 0.6689 - val_loss: 1.8608 - val_accuracy: 0.4730\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9225 - accuracy: 0.6739 - val_loss: 1.9048 - val_accuracy: 0.4974\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.9057 - accuracy: 0.6791 - val_loss: 1.9673 - val_accuracy: 0.4976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f168eeb53d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\" \n",
        "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
        "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
        "validation accuracy at each epoch.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Create the neural network\n",
        "nn = Sequential()\n",
        "nn.add(Dense(1024, input_shape = (3072, ), activation = 'relu'))\n",
        "nn.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "# Create our optimizer\n",
        "sgd = SGD(learning_rate = 0.1)\n",
        "\n",
        "# Selection loss function and metrics for accuracy\n",
        "nn.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
        "          metrics = 'accuracy')\n",
        "\n",
        "# Runn neural network\n",
        "nn.fit(train_x, train_y, shuffle = True, epochs = 50, \n",
        "      validation_data = (test_x, test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYKAeyQrdD5"
      },
      "source": [
        "#### Question 2\n",
        "\n",
        "Complete the following table on the design choices for your MLP \n",
        "(3 MARKS):\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            | SGD         | Specified in question |\n",
        "| # of hidden layers   | 1           | Specified in question |\n",
        "| # of hidden neurons  | 1024        | Specified in question |\n",
        "| Hid layer activation | ReLu        | Common activation function in hidden layer to introduce non-linearity|\n",
        "| # of output neurons  | 10          |There are 10 possible categories for the solution, and the y dataset is an array of size 10|\n",
        "| Output activation    |softmax      |For multiclass  classification problems, and outputs a statistical distribution for the 10 output|\n",
        "| lr                   |0.1          |Start with low learning rate to prevent huge swings in initial learnings |\n",
        "| momentum             |None    |To not include momentum for the first run|\n",
        "| decay                |None |Used for slowing down learning rate overtime to prevent overfitting. As unsure if there will be even after 50 epochs, to not include first|\n",
        "| loss                 |categorical cross entropy|For multiclass classification problems|\n",
        "\n",
        "*For TA: ___ / 3* <br>\n",
        "*Code:  ____/ 5* <br>\n",
        "**TOTAL: ____ / 8** <br>\n",
        "\n",
        "#### Question 3:\n",
        "\n",
        "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer (5 MARKS)\n",
        "\n",
        "**The final training accuracy was 0.6791 and validation accuracy was 0.4976. There is likely some overfitting as while the training accuracy of the model was still slowly improving in the last few epochs, the validation accuracy has not improved beyond 0.5.**\n",
        "\n",
        "*FOR GRADER: ______ / 5*\n",
        "\n",
        "### 3.3 Experimenting with the MLP\n",
        "\n",
        "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOUKedBprdD6",
        "outputId": "9efceca1-539d-488d-b190-77ff4137c049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.8545 - accuracy: 0.3301 - val_loss: 1.6996 - val_accuracy: 0.3831\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6728 - accuracy: 0.3982 - val_loss: 1.6625 - val_accuracy: 0.3985\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5939 - accuracy: 0.4264 - val_loss: 1.5979 - val_accuracy: 0.4325\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5429 - accuracy: 0.4459 - val_loss: 1.5581 - val_accuracy: 0.4519\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5006 - accuracy: 0.4618 - val_loss: 1.4796 - val_accuracy: 0.4743\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.4690 - accuracy: 0.4749 - val_loss: 1.4775 - val_accuracy: 0.4764\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4379 - accuracy: 0.4844 - val_loss: 1.4771 - val_accuracy: 0.4726\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.4182 - accuracy: 0.4938 - val_loss: 1.4764 - val_accuracy: 0.4781\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3949 - accuracy: 0.5008 - val_loss: 1.4388 - val_accuracy: 0.4871\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.3712 - accuracy: 0.5093 - val_loss: 1.4482 - val_accuracy: 0.4878\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.3482 - accuracy: 0.5154 - val_loss: 1.4280 - val_accuracy: 0.4912\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3262 - accuracy: 0.5241 - val_loss: 1.4231 - val_accuracy: 0.4894\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3139 - accuracy: 0.5292 - val_loss: 1.4746 - val_accuracy: 0.4708\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2962 - accuracy: 0.5375 - val_loss: 1.4353 - val_accuracy: 0.4946\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2784 - accuracy: 0.5399 - val_loss: 1.4654 - val_accuracy: 0.4833\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2655 - accuracy: 0.5431 - val_loss: 1.4426 - val_accuracy: 0.4937\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2478 - accuracy: 0.5502 - val_loss: 1.4721 - val_accuracy: 0.4941\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2370 - accuracy: 0.5560 - val_loss: 1.4784 - val_accuracy: 0.4809\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.2167 - accuracy: 0.5626 - val_loss: 1.4825 - val_accuracy: 0.4922\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1999 - accuracy: 0.5703 - val_loss: 1.4779 - val_accuracy: 0.4905\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.1920 - accuracy: 0.5719 - val_loss: 1.4740 - val_accuracy: 0.4946\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1786 - accuracy: 0.5736 - val_loss: 1.4929 - val_accuracy: 0.4915\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.1684 - accuracy: 0.5808 - val_loss: 1.4750 - val_accuracy: 0.4993\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1537 - accuracy: 0.5845 - val_loss: 1.4756 - val_accuracy: 0.5042\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1436 - accuracy: 0.5878 - val_loss: 1.5126 - val_accuracy: 0.4918\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1326 - accuracy: 0.5904 - val_loss: 1.4876 - val_accuracy: 0.4953\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1193 - accuracy: 0.5979 - val_loss: 1.5062 - val_accuracy: 0.4973\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1109 - accuracy: 0.5983 - val_loss: 1.5362 - val_accuracy: 0.5042\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0971 - accuracy: 0.6049 - val_loss: 1.5648 - val_accuracy: 0.4869\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.0872 - accuracy: 0.6075 - val_loss: 1.5815 - val_accuracy: 0.4851\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0821 - accuracy: 0.6087 - val_loss: 1.5982 - val_accuracy: 0.4787\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.0680 - accuracy: 0.6134 - val_loss: 1.5812 - val_accuracy: 0.4924\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0599 - accuracy: 0.6181 - val_loss: 1.5570 - val_accuracy: 0.4944\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0451 - accuracy: 0.6221 - val_loss: 1.5848 - val_accuracy: 0.4928\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0419 - accuracy: 0.6240 - val_loss: 1.6141 - val_accuracy: 0.4836\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0342 - accuracy: 0.6280 - val_loss: 1.6201 - val_accuracy: 0.4836\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.0248 - accuracy: 0.6299 - val_loss: 1.6101 - val_accuracy: 0.4955\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0210 - accuracy: 0.6304 - val_loss: 1.6344 - val_accuracy: 0.4880\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0072 - accuracy: 0.6335 - val_loss: 1.6845 - val_accuracy: 0.4884\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9984 - accuracy: 0.6379 - val_loss: 1.6810 - val_accuracy: 0.4854\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9928 - accuracy: 0.6399 - val_loss: 1.6916 - val_accuracy: 0.4866\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.9868 - accuracy: 0.6443 - val_loss: 1.6691 - val_accuracy: 0.4901\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9826 - accuracy: 0.6452 - val_loss: 1.7047 - val_accuracy: 0.4843\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.9741 - accuracy: 0.6478 - val_loss: 1.6866 - val_accuracy: 0.4901\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9604 - accuracy: 0.6484 - val_loss: 1.6687 - val_accuracy: 0.4978\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9469 - accuracy: 0.6580 - val_loss: 1.7211 - val_accuracy: 0.4924\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.9532 - accuracy: 0.6574 - val_loss: 1.7268 - val_accuracy: 0.4886\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9413 - accuracy: 0.6594 - val_loss: 1.7139 - val_accuracy: 0.4950\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9337 - accuracy: 0.6657 - val_loss: 1.7133 - val_accuracy: 0.4905\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9285 - accuracy: 0.6653 - val_loss: 1.8056 - val_accuracy: 0.4898\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.9188 - accuracy: 0.6672 - val_loss: 1.7530 - val_accuracy: 0.4870\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9142 - accuracy: 0.6716 - val_loss: 1.7850 - val_accuracy: 0.4874\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.9165 - accuracy: 0.6695 - val_loss: 1.8387 - val_accuracy: 0.4813\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9052 - accuracy: 0.6725 - val_loss: 1.8553 - val_accuracy: 0.4858\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9015 - accuracy: 0.6722 - val_loss: 1.8653 - val_accuracy: 0.4845\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8914 - accuracy: 0.6775 - val_loss: 1.8823 - val_accuracy: 0.4718\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8907 - accuracy: 0.6786 - val_loss: 1.9149 - val_accuracy: 0.4754\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8801 - accuracy: 0.6798 - val_loss: 1.9263 - val_accuracy: 0.4824\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8791 - accuracy: 0.6828 - val_loss: 1.9297 - val_accuracy: 0.4819\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8738 - accuracy: 0.6834 - val_loss: 1.8899 - val_accuracy: 0.4869\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8655 - accuracy: 0.6870 - val_loss: 1.9768 - val_accuracy: 0.4767\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8642 - accuracy: 0.6882 - val_loss: 1.9095 - val_accuracy: 0.4935\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8535 - accuracy: 0.6924 - val_loss: 1.9364 - val_accuracy: 0.4875\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8595 - accuracy: 0.6909 - val_loss: 1.9910 - val_accuracy: 0.4825\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8505 - accuracy: 0.6926 - val_loss: 1.9512 - val_accuracy: 0.4747\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8438 - accuracy: 0.6920 - val_loss: 2.0161 - val_accuracy: 0.4820\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8339 - accuracy: 0.6993 - val_loss: 2.1324 - val_accuracy: 0.4774\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8316 - accuracy: 0.6994 - val_loss: 1.9948 - val_accuracy: 0.4849\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8299 - accuracy: 0.7009 - val_loss: 2.1202 - val_accuracy: 0.4721\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8294 - accuracy: 0.7002 - val_loss: 2.0057 - val_accuracy: 0.4861\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8180 - accuracy: 0.7024 - val_loss: 2.0418 - val_accuracy: 0.4821\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8160 - accuracy: 0.7038 - val_loss: 2.1292 - val_accuracy: 0.4747\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8081 - accuracy: 0.7072 - val_loss: 2.0634 - val_accuracy: 0.4735\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8095 - accuracy: 0.7086 - val_loss: 2.1455 - val_accuracy: 0.4833\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8109 - accuracy: 0.7054 - val_loss: 2.1443 - val_accuracy: 0.4846\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8025 - accuracy: 0.7090 - val_loss: 2.1532 - val_accuracy: 0.4799\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7946 - accuracy: 0.7119 - val_loss: 2.1016 - val_accuracy: 0.4838\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7956 - accuracy: 0.7145 - val_loss: 2.1364 - val_accuracy: 0.4726\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 34s 21ms/step - loss: 0.7906 - accuracy: 0.7129 - val_loss: 2.2050 - val_accuracy: 0.4699\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7844 - accuracy: 0.7148 - val_loss: 2.2362 - val_accuracy: 0.4799\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7871 - accuracy: 0.7185 - val_loss: 2.1820 - val_accuracy: 0.4849\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7768 - accuracy: 0.7182 - val_loss: 2.2009 - val_accuracy: 0.4736\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7822 - accuracy: 0.7156 - val_loss: 2.2675 - val_accuracy: 0.4652\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7768 - accuracy: 0.7199 - val_loss: 2.2010 - val_accuracy: 0.4732\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7717 - accuracy: 0.7220 - val_loss: 2.3048 - val_accuracy: 0.4784\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7644 - accuracy: 0.7245 - val_loss: 2.2575 - val_accuracy: 0.4688\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7624 - accuracy: 0.7233 - val_loss: 2.3426 - val_accuracy: 0.4681\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7625 - accuracy: 0.7240 - val_loss: 2.3558 - val_accuracy: 0.4658\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7488 - accuracy: 0.7290 - val_loss: 2.2711 - val_accuracy: 0.4735\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7572 - accuracy: 0.7266 - val_loss: 2.3284 - val_accuracy: 0.4846\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7489 - accuracy: 0.7292 - val_loss: 2.4492 - val_accuracy: 0.4744\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7425 - accuracy: 0.7325 - val_loss: 2.3450 - val_accuracy: 0.4717\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7466 - accuracy: 0.7310 - val_loss: 2.4072 - val_accuracy: 0.4724\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7426 - accuracy: 0.7294 - val_loss: 2.4554 - val_accuracy: 0.4686\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7355 - accuracy: 0.7323 - val_loss: 2.4973 - val_accuracy: 0.4739\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7349 - accuracy: 0.7352 - val_loss: 2.4512 - val_accuracy: 0.4774\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7406 - accuracy: 0.7327 - val_loss: 2.4285 - val_accuracy: 0.4834\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 34s 21ms/step - loss: 0.7291 - accuracy: 0.7361 - val_loss: 2.5066 - val_accuracy: 0.4625\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7230 - accuracy: 0.7367 - val_loss: 2.4767 - val_accuracy: 0.4729\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7211 - accuracy: 0.7404 - val_loss: 2.5859 - val_accuracy: 0.4711\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61637bb1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\"\"\"\n",
        "Cut and paste your code from Section 3.2 below, then modify it to get\n",
        "much better results than what you had earlier. E.g. increase the number of\n",
        "nodes in the hidden layer, increase the number of hidden layers,\n",
        "change the optimizer, etc. \n",
        "\n",
        "Train for 100 epochs.\n",
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create the neural network\n",
        "nn = Sequential()\n",
        "nn.add(Dense(512, input_shape = (3072, ), activation = 'relu'))\n",
        "nn.add(Dense(256, activation = 'relu'))\n",
        "nn.add(Dense(128, activation = 'relu'))\n",
        "nn.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "# Create our optimizer\n",
        "optimizer = Adam()\n",
        "\n",
        "# Selection loss function and metrics for accuracy\n",
        "nn.compile(loss='categorical_crossentropy', optimizer=optimizer, \n",
        "          metrics = 'accuracy')\n",
        "\n",
        "# Runn neural network\n",
        "nn.fit(train_x, train_y, shuffle = True, epochs = 100, \n",
        "      validation_data = (test_x, test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2HUUwSnrdD6"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 4:\n",
        "\n",
        "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (3 MARKS)\n",
        "\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            | Adam        | It has adaptive learning rate and will increase/decrease the rate according to the gradient last few gradients |\n",
        "| # of hidden layers   |  2          | To help to filter out the most important data in the first layer before selecting the category in the second |\n",
        "| # neurons(layer1)    | 512      |Reduce the num of neurons to reduce complexity of the model and hopefully reduce overfitting |\n",
        "| Hid layer1 activation| relu        | Common activation function for hidden layers |\n",
        "| # neurons(layer2)    | 256         | add a 2nd layer for more dimensionality to the model (have multiple lines as it is multi-class classification problem) |\n",
        "| Hid layer2 activation| relu        | Common activation function for hidden layers |\n",
        "| # neurons(layer2)    | 128         | add a 3rd layer for more dimensionality to the model (have multiple lines as it is multi-class classification problem) |\n",
        "| Hid layer2 activation| relu        | Common activation function for hidden layers |\n",
        "| # of output neurons  | 10          | 10 categories of images |\n",
        "| Output activation    | softmax     | For multiclass classification problem   |\n",
        "| lr                   | none        | not required for Adam Optimizer as it can adapt |\n",
        "| momentum             | none        | not required for Adam Optimizer as it can adapt |\n",
        "| decay                | none        | not required for Adam Optimizer as it can adapt |\n",
        "| loss                 | category cross entropy |For multiclass classification problem |\n",
        "\n",
        "*FOR GRADER: _____ / 3 * <br>\n",
        "*CODE: ______ / 5 *<br>\n",
        "\n",
        "***TOTAL: ______ / 8***\n",
        "\n",
        "#### Question 5\n",
        "\n",
        "What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer (5 MARKS)\n",
        "\n",
        "**The final training and validation accuracy is 0.7404 and 0.4711 respectively. There is no considerable improvement as the validation accuracy is still as low, capping at around 0.49 - 0.5. While the model is able to achieve a higher training accuracy this is mainly due to overfitting. The validation accuracy has stagnated since the 20th epoch and slowly came down after the 35th epoch.**\n",
        "\n",
        "*FOR GRADER: ______ / 5 *\n",
        "\n",
        "#### Question 6\n",
        "\n",
        "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets. (3 MARKS)\n",
        "\n",
        "**The accurancy of the model remain quite low despite adding additional layers to the neural network. We might need to further reduce the complexity of the model to see if it could reduce overfitting. It might be important to use other techniques such as convolution to simplify the input dataset before training the model to improve the results. By using convolution, we can do some feature selection first and use the new features to help with the classification problem (e.g. finding a wheel which can show that it is a automobile or truck or plane).**\n",
        "\n",
        "**Overfitting also seem to be an unavoidable issue even by changing the complexity and layers in the neural network. The model will tend towards memorising the dataset than the features from the dataset which lead to it not being able to predict well of outcome on hidden datasets.**\n",
        "\n",
        "*FOR GRADER: _______ /3*\n",
        "\n",
        "----\n",
        "\n",
        "## 4. Creating a CNN for the MNIST Data Set\n",
        "\n",
        "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
        "\n",
        "### 4.1 Loading the MNIST Dataset\n",
        "\n",
        "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuKYEZ5HrdD7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def load_mnist():\n",
        "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
        "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
        "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
        "\n",
        "    train_x=train_x.astype('float32')\n",
        "    test_x = test_x.astype('float32')\n",
        "    \n",
        "    train_x /= 255.0\n",
        "    test_x /= 255.0\n",
        "        \n",
        "    train_y = to_categorical(train_y, 10)\n",
        "    test_y = to_categorical(test_y, 10)\n",
        "        \n",
        "    return (train_x, train_y), (test_x, test_y) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S9VkZ6OrdD8"
      },
      "source": [
        "### 4.2 Building the CNN\n",
        "\n",
        "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
        "\n",
        "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zbn4CZUrdD9"
      },
      "outputs": [],
      "source": [
        "# load_model loads a model from a hd5 file.\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "MODEL_NAME = 'mnist-cnn.hd5'\n",
        "\n",
        "def buildmodel(model_name):\n",
        "    if os.path.exists(model_name):\n",
        "        model = load_model(model_name)                                                                                             \n",
        "    else:\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(5,5),\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "        model.add(Flatten()) # Question 9\n",
        "        model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dropout(0.1))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUE_P_5jrdD9"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 7\n",
        "\n",
        "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
        "\n",
        "```\n",
        "        model.add(Conv2D(32, kernel_size=(5,5),\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
        "```\n",
        "\n",
        "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? (4 MARKS)\n",
        "\n",
        "**The input_shape is set to same size of one data point of a 28x28 image. This means that each image is one datapoint. Padding = same refers to that after convolution, we want to retain the size of the output as the input (i.e. 28x28x1 in this case).**\n",
        "\n",
        "*FOR GRADER: ______ / 4*\n",
        "\n",
        "#### Question 8\n",
        "\n",
        "The second layer is the MaxPooling2D layer shown below:\n",
        "\n",
        "```\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
        "```\n",
        "\n",
        "What other types of pooling layers are available? What does 'strides = 2' mean? (3 MARKS)\n",
        "\n",
        "**The other type of pooling is average pooling in which you take the mean values of a region in the input and convert it into one output as compared to maxpooling in which you take the max values of a region and convert it into the output. Strides refers to the number of steps you take each time when you pool. Strides = 2 means that you move by 2 steps in the array either right or down. As the pool_size is 2x2, there is no overlapping in the pooling of each region.**\n",
        "\n",
        "*FOR GRADER: _____ / 3*\n",
        "\n",
        "\n",
        "#### Question 9\n",
        "\n",
        "What does the \"Flatten\" layer here do? Why is it needed?\n",
        "\n",
        "```\n",
        "        model.add(Flatten()) # Question 9\n",
        "```\n",
        "**Flatten refers to changing the input array into 1 1D array which in this case is a 32x32=784 element vector. This is required before passing to the dense layer as the layer dense can only read a 1D array.**\n",
        "\n",
        "*FOR GRADER: ____ / 2*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "### 4.3 Training the CNN\n",
        "\n",
        "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
        "\n",
        "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
        "    \n",
        "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
        "    \n",
        "\n",
        "Let's look at the code to see how training is done, and how callbacks are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YWI9Kn2rdD9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
        "\n",
        "    model.compile(optimizer=SGD(lr=0.01, momentum=0.7), \n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    savemodel = ModelCheckpoint(model_name)\n",
        "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
        "\n",
        "    print(\"Starting training.\")\n",
        "\n",
        "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
        "    validation_data=(test_x, test_y), shuffle=True,\n",
        "    epochs=epochs, \n",
        "    callbacks=[savemodel, stopmodel])\n",
        "\n",
        "    print(\"Done. Now evaluating.\")\n",
        "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
        "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1qB-7irdD-"
      },
      "source": [
        "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
        "\n",
        "```\n",
        "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
        "    validation_data=(test_x, test_y), shuffle=True,\n",
        "    epochs=epochs, \n",
        "    callbacks=[savemodel, stopmodel])\n",
        "```\n",
        "\n",
        "----\n",
        "\n",
        "#### Question 10.\n",
        "\n",
        "What does do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
        "\n",
        "```\n",
        "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
        "```\n",
        "**Min_delta refers to the minimum change in the monitored matric to qualify as an improvement. If the change is lesser than the min_delta value, then it will not be considered an improvement. Patience is the parameter to stop the training when a set number of epochs with no improvement is reached. So for this case, after 10 epochs of no improvement, the model will stop running.**\n",
        "\n",
        "---\n",
        "\n",
        "### 4.4 Putting it together.\n",
        "\n",
        "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48NoaIcOrdD-",
        "outputId": "7ec54681-035b-4c36-b80c-119f43b22832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training.\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8903"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 243s 129ms/step - loss: 0.3465 - accuracy: 0.8903 - val_loss: 0.0784 - val_accuracy: 0.9734\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9771"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 246s 131ms/step - loss: 0.0742 - accuracy: 0.9771 - val_loss: 0.0515 - val_accuracy: 0.9832\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9848"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 242s 129ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.0381 - val_accuracy: 0.9866\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9884"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 243s 130ms/step - loss: 0.0360 - accuracy: 0.9884 - val_loss: 0.0350 - val_accuracy: 0.9885\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9917"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 242s 129ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0331 - val_accuracy: 0.9892\n",
            "Done. Now evaluating.\n",
            "313/313 [==============================] - 8s 27ms/step - loss: 0.0331 - accuracy: 0.9892\n",
            "Test accuracy: 0.99, loss: 0.03\n"
          ]
        }
      ],
      "source": [
        "    (train_x, train_y),(test_x, test_y) = load_mnist()\n",
        "    model = buildmodel(MODEL_NAME)\n",
        "    train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A421lgiUrdD_"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 11.\n",
        "\n",
        "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)? (3 MARKS)\n",
        "\n",
        "**The CNN helps to reduce the numbers of input point into the dense layer and hence complexity of the model. How the reduced number of input point is also learnt through gradient descent and improved using the dataset. However, convolution is meant to extract higher level features in a dataset. If there are no strong higher level features in the dataset to help train the model, it might make the model less accurate.**\n",
        "\n",
        "*FOR TA: ______ / 3*\n",
        "\n",
        "## 5. Making a CNN for the CIFAR-10 Dataset\n",
        "\n",
        "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the MNIST-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
        "\n",
        "---\n",
        "\n",
        "#### Question 12.\n",
        "\n",
        "Summarize your design in the table below (the actual coding cell comes after this):\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            |Adam        | It has adaptive learning rate and will increase/decrease the rate according to the gradient last few gradients |\n",
        "| Input shape          |32x32x3      | Set to the same shape as the raw dataset|\n",
        "| First layer          |Conv3D       |To perform first feature selection and select the important data |\n",
        "| Second layer         |Maxpooling   |Pool to reduce the dataset|\n",
        "| Third layer          |Conv3D       |To perform a feature selections of the simple features from the 1st convolution layer and further simpilfy the data|\n",
        "| Fourth layer         |Maxpooling   |Pool again to reduce the dataset|\n",
        "| Fifith layer         |Flatten      |To flatten before sending to dense layer|\n",
        "| Dense layer          |1024         |To be comparable to the non-CNN model trained in part 1|\n",
        "\n",
        "\n",
        "*FOR TA:*\n",
        "*Table: ________ / 3* <br>\n",
        "*Code: _________/ 7* <br>\n",
        "**TOTAL: _______ / 10** <br>\n",
        "\n",
        "---\n",
        "\n",
        "***TOTAL: _______ / 55***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjMNhSyNrdD_",
        "outputId": "6a743b32-9e8f-40a0-defb-d93eea122f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.4811 - accuracy: 0.8332"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 194s 124ms/step - loss: 0.4811 - accuracy: 0.8332 - val_loss: 1.1730 - val_accuracy: 0.6599\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.8703"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 188s 120ms/step - loss: 0.3722 - accuracy: 0.8703 - val_loss: 1.1783 - val_accuracy: 0.6586\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.8996"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 205s 131ms/step - loss: 0.2921 - accuracy: 0.8996 - val_loss: 1.4620 - val_accuracy: 0.6667\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9135"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 187s 120ms/step - loss: 0.2576 - accuracy: 0.9135 - val_loss: 1.4816 - val_accuracy: 0.6693\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9266"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 194s 124ms/step - loss: 0.2182 - accuracy: 0.9266 - val_loss: 1.6526 - val_accuracy: 0.6716\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 1.6526 - accuracy: 0.6716\n",
            "Test accuracy: 0.67, loss: 1.65\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Write your code for your CNN for the CIFAR-10 dataset here. \n",
        "\n",
        "Note: train_x, train_y, test_x, test_y were changed when we called \n",
        "load_mnist in the previous section. You will now need to call load_cifar10\n",
        "again.\n",
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "#load the cifar10 data\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "train_x = train_x.astype('float32')\n",
        "test_x = test_x.astype('float32')\n",
        "train_x /= 255.0\n",
        "test_x /= 255.0\n",
        "train_y = to_categorical(train_y,10)\n",
        "test_y = to_categorical(test_y, 10)\n",
        "\n",
        "#code for the CNN model\n",
        "model_name = 'cifar.hd5'\n",
        "\n",
        "if os.path.exists(model_name):\n",
        "    model = load_model(model_name)                                                                                             \n",
        "else:\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(5,5), activation='relu', input_shape=(32, 32, 3), padding='same')) \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2)) \n",
        "    model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model.add(Flatten()) \n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), \n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Create a place for saving the checkpoints\n",
        "savemodel = ModelCheckpoint(model_name)\n",
        "\n",
        "#Run the model\n",
        "model.fit(x=train_x, y=train_y, batch_size=32, validation_data=(test_x, test_y), shuffle=True, epochs=5, callbacks=[savemodel])\n",
        "\n",
        "#output the final results\n",
        "loss, acc = model.evaluate(x=test_x, y=test_y)\n",
        "print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFyaOUdbrdD_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}